{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Tokenization**"
      ],
      "metadata": {
        "id": "riW-3pOrZxlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In SpaCy, tokenization is part of the language model pipeline, and tokens can be accessed using [token.text for token in doc].**"
      ],
      "metadata": {
        "id": "yb74KUb6aL8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the SpaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process the text\n",
        "text = \"This is an example sentence. Tokenize this sentence using SpaCy.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract tokens\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(spacy_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4dpIQLAZupY",
        "outputId": "796bb98b-7064-4551-b5d2-caaadf0ecaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', '.', 'Tokenize', 'this', 'sentence', 'using', 'SpaCy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We load SpaCy’s model, pass the text to `nlp()` to tokenize it, extract the tokens using `[token.text for token in doc]`, and print them.**"
      ],
      "metadata": {
        "id": "wj65kD9CavB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SpaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# Tokenize the text\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOEWHUz5XXb9",
        "outputId": "d8ccf3f9-177c-4021-badb-5ba9b8f3b736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Lowercasing**\n",
        "\n",
        "Converting all text to lowercase to make it case-insensitive. To lowercase the tokens in a list using SpaCy, you can use the built-in lower() method for each token in the Doc object:\n"
      ],
      "metadata": {
        "id": "zZ-mPYb7Xl5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# Tokenize the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Lowercase the tokens\n",
        "lowercased_tokens = [token.text.lower() for token in doc]\n",
        "\n",
        "print(\"Lowercased tokens:\", lowercased_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr6NIkmnXoZh",
        "outputId": "35a1e305-69ab-43ba-f329-f23b3b5bb6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased tokens: ['natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Remove Punctuation**\n",
        "Removing punctuation marks simplifies the text and makes it easier to process. To remove punctuation from a list of tokens using SpaCy, you can check if each token is a punctuation character using the is_punct attribute. Here is an example of how to do this:"
      ],
      "metadata": {
        "id": "Ksyt33EKZZrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# Tokenize the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Remove punctuation\n",
        "filtered_tokens = [token.text for token in doc if not token.is_punct]\n",
        "\n",
        "print(\"Tokens without punctuation:\", filtered_tokens)\n"
      ],
      "metadata": {
        "id": "BuNrz5kiZk24",
        "outputId": "b553e9bb-eeef-4e9b-c58e-0a18cbb960c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without punctuation: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'natural', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Remove Stop Words**\n",
        "\n",
        "Removing common words that do not add significant meaning to the text, such as “a,” “an,” and “the,” is an important step in text processing. To remove common stop words from a list of tokens using SpaCy, you can use the is_stop attribute of each token. Here is an example of how to do this:\n",
        "\n",
        "**NOTE: In SpaCy, you don't need to download stop words separately like you do in NLTK. The stop words are included with the language model when you load it. Therefore, you can simply remove the nltk.download('stopwords') line altogether**"
      ],
      "metadata": {
        "id": "kuZ4tcITc4PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# Tokenize the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "print(\"Tokens without stop words:\", filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1WfRbbfdZyv",
        "outputId": "1063d372-1fbb-4f02-8dce-133eb5b54c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without stop words: ['Natural', 'language', 'processing', 'field', 'artificial', 'intelligence', 'deals', 'interaction', 'computers', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Remove extra whitespace**\n",
        "\n",
        "In SpaCy, you can handle whitespace cleaning directly with Python string methods since SpaCy doesn’t have a built-in function for this specific task. Here's the SpaCy version for removing extra whitespace:"
      ],
      "metadata": {
        "id": "ZXkZCy-2fjDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text with extra white space\n",
        "text = \"  Natural   language processing   is   a field   of artificial intelligence   that deals with the interaction between computers and human   (natural)   language.   \"\n",
        "\n",
        "# Remove leading and trailing white space\n",
        "text = text.strip()\n",
        "\n",
        "# Replace multiple consecutive white space characters with a single space\n",
        "text = \" \".join(text.split())\n",
        "\n",
        "print(\"Cleaned text:\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p2hOkn7f84R",
        "outputId": "4e5e35f5-2676-4088-8ead-567f696827e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text: Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Remove URLs**\n",
        "\n",
        "\n",
        "To remove URLs in SpaCy, you would still use regular expressions (as SpaCy doesn't have built-in URL detection), but the tokenization can be handled by SpaCy if you want to work with tokens afterward."
      ],
      "metadata": {
        "id": "sDT5m-gvgTNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Input text with URLs\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. Check out this article for more information: https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
        "\n",
        "# Define a regular expression pattern to match URLs\n",
        "pattern = r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
        "\n",
        "# Replace URLs with an empty string\n",
        "cleaned_text = re.sub(pattern, \"\", text).strip()\n",
        "\n",
        "# Print the cleaned text without URLs\n",
        "print(\"Text without URLs:\", cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANuPixWEgazi",
        "outputId": "954d4ba5-42ff-4cbd-d240-912e71aab988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without URLs: Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. Check out this article for more information:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Remove HTML code**\n",
        "\n",
        "In SpaCy, you'd still use regular expressions to remove HTML tags as SpaCy doesn't have built-in functionality for stripping HTML. Here's how you can convert your NLTK example into SpaCy:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "65WIX8W8g2sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Input text with HTML code\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. <b>This is an example of bold text.</b>\"\n",
        "\n",
        "# Define a regular expression pattern to match HTML tags\n",
        "pattern = r\"<[^>]+>\"\n",
        "\n",
        "# Replace HTML tags with an empty string\n",
        "cleaned_text = re.sub(pattern, \"\", text).strip()\n",
        "\n",
        "# Optionally process the cleaned text using SpaCy (if further processing is needed)\n",
        "doc = nlp(cleaned_text)\n",
        "\n",
        "# Print the cleaned text without HTML\n",
        "print(\"Text without HTML code:\", cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7hVIxLDhWJG",
        "outputId": "5826d4a4-6129-4cb1-e32f-511bdc0ea9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without HTML code: Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. This is an example of bold text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.  Lemmatization**\n",
        "\n",
        "NOTE: In SpaCy, stemming is not directly available because it uses lemmatization instead, which is more sophisticated and accurate. Lemmatisation returns the base or dictionary form of a word, unlike stemming, which often cuts off prefixes or suffixes. Here's how you can achieve a similar effect using lemmatization in SpaCy:\n"
      ],
      "metadata": {
        "id": "4Sjw4O94hlCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# Tokenize and process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Lemmatize each token\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"Lemmatized tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0HFvJUVh4Pa",
        "outputId": "fb0cb021-4d36-48ac-ede4-35ed903c8249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens: ['natural', 'language', 'processing', 'be', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deal', 'with', 'the', 'interaction', 'between', 'computer', 'and', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9.  Part-of-speech tagging**\n",
        "\n",
        "In SpaCy, there’s no need for separate downloads for POS tagging. The language model (e.g., `en_core_web_sm`) includes built-in features like tokenization, POS tagging, and lemmatization.\n",
        "\n",
        "After loading the model (`nlp = spacy.load('en_core_web_sm')`), you can immediately perform POS tagging. The text is processed using `doc = nlp(text)` to create a `Doc` object, and you can access each token's POS tag with `token.pos_`."
      ],
      "metadata": {
        "id": "i62cH0ECibJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# Tokenize and process the text using SpaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tag each token with its POS tag\n",
        "tagged_tokens = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "print(\"Tagged tokens:\", tagged_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh3MEvzvi_Mp",
        "outputId": "c08fd6e7-b3b8-4ca0-f911-f293b4da4b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged tokens: [('Natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'NOUN'), ('is', 'AUX'), ('a', 'DET'), ('field', 'NOUN'), ('of', 'ADP'), ('artificial', 'ADJ'), ('intelligence', 'NOUN'), ('that', 'PRON'), ('deals', 'VERB'), ('with', 'ADP'), ('the', 'DET'), ('interaction', 'NOUN'), ('between', 'ADP'), ('computers', 'NOUN'), ('and', 'CCONJ'), ('human', 'ADJ'), ('(', 'PUNCT'), ('natural', 'ADJ'), (')', 'PUNCT'), ('language', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Named Entity Recognition**\n",
        "\n",
        "In SpaCy, no separate downloads are needed for Named Entity Recognition (NER) since the language model (e.g., `en_core_web_sm`) includes NER functionality.\n",
        "\n",
        "Load the model with `nlp = spacy.load('en_core_web_sm')`, then process text using `doc = nlp(text)`. Access named entities with `doc.ents`, which gives a list of entities as tuples (entity text, label) like \"PERSON\" or \"ORG\"."
      ],
      "metadata": {
        "id": "fxVBP8e6jJoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. John Smith works at Google in New York.\"\n",
        "\n",
        "# Process the text using SpaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract named entities\n",
        "named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "print(\"Named entities:\", named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtdWl3Lsjn2n",
        "outputId": "7fd04041-a72f-41b5-c5bd-069391ebbd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities: [('John Smith', 'PERSON'), ('Google', 'ORG'), ('New York', 'GPE')]\n"
          ]
        }
      ]
    }
  ]
}